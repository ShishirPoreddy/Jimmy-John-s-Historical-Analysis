{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874a71b2-bbd0-4ee5-ad0a-b0c74a1b805e",
   "metadata": {},
   "source": [
    "Shishir Poreddy - Jimmy John's - ECON433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c08ece-3a60-49f0-a8a5-ef4701ffc031",
   "metadata": {},
   "source": [
    "Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3c7b54-779b-4ace-b576-d18ebebac801",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['placekey', 'safegraph_brand_ids', 'brands', 'top_category',\n",
      "       'naics_code', 'city', 'region', 'postal_code', 'date_range_start',\n",
      "       'date_range_end', 'raw_visit_counts', 'visits_by_day'],\n",
      "      dtype='object')\n",
      "Index(['placekey', 'safegraph_brand_ids', 'brands', 'top_category',\n",
      "       'naics_code', 'city', 'region', 'postal_code', 'date_range_start',\n",
      "       'date_range_end', 'raw_visit_counts', 'visits_by_day'],\n",
      "      dtype='object')\n",
      "Index(['placekey', 'safegraph_brand_ids', 'brands', 'top_category',\n",
      "       'naics_code', 'city', 'region', 'postal_code', 'date_range_start',\n",
      "       'date_range_end', 'raw_visit_counts', 'visits_by_day'],\n",
      "      dtype='object')\n",
      "Index(['placekey', 'safegraph_brand_ids', 'brands', 'top_category',\n",
      "       'naics_code', 'city', 'region', 'postal_code', 'date_range_start',\n",
      "       'date_range_end', 'raw_visit_counts', 'visits_by_day'],\n",
      "      dtype='object')\n",
      "Index(['placekey', 'safegraph_brand_ids', 'brands', 'top_category',\n",
      "       'naics_code', 'city', 'region', 'postal_code', 'date_range_start',\n",
      "       'date_range_end', 'raw_visit_counts', 'visits_by_day'],\n",
      "      dtype='object')\n",
      "/Users/shishirporeddy/ECON433/weekly_patterns_2018_sample.csv.gz: 0 rows\n",
      "/Users/shishirporeddy/ECON433/weekly_patterns_2019_sample.csv.gz: 0 rows\n",
      "/Users/shishirporeddy/ECON433/weekly_patterns_2020_sample.csv.gz: 0 rows\n",
      "/Users/shishirporeddy/ECON433/weekly_patterns_2021_sample.csv.gz: 0 rows\n",
      "/Users/shishirporeddy/ECON433/weekly_patterns_2022_sample.csv.gz: 0 rows\n",
      "5196505\n",
      "              placekey                        safegraph_brand_ids  \\\n",
      "0  22c-223@8f6-sk4-mx5  SG_BRAND_a04f493d784143c9b21b2689e32fd0d1   \n",
      "1  225-222@5s8-nxw-8n5  SG_BRAND_a04f493d784143c9b21b2689e32fd0d1   \n",
      "2  224-222@63g-4cn-bc5  SG_BRAND_a04f493d784143c9b21b2689e32fd0d1   \n",
      "3  225-222@63v-7dz-q4v  SG_BRAND_a04f493d784143c9b21b2689e32fd0d1   \n",
      "4  225-222@5yv-gwr-d9z  SG_BRAND_a04f493d784143c9b21b2689e32fd0d1   \n",
      "\n",
      "         brands                         top_category naics_code  \\\n",
      "0  Jimmy John's  Restaurants and Other Eating Places     722513   \n",
      "1  Jimmy John's  Restaurants and Other Eating Places     722513   \n",
      "2  Jimmy John's  Restaurants and Other Eating Places     722513   \n",
      "3  Jimmy John's  Restaurants and Other Eating Places     722513   \n",
      "4  Jimmy John's  Restaurants and Other Eating Places     722513   \n",
      "\n",
      "             city region postal_code           date_range_start  \\\n",
      "0         Flowood     MS       39232  2018-01-01T00:00:00-06:00   \n",
      "1  Chippewa Falls     WI       54729  2018-01-01T00:00:00-06:00   \n",
      "2   Winston Salem     NC       27105  2018-01-01T00:00:00-05:00   \n",
      "3         Wooster     OH       44691  2018-01-01T00:00:00-05:00   \n",
      "4       Henderson     NV       89015  2018-01-01T00:00:00-08:00   \n",
      "\n",
      "              date_range_end raw_visit_counts     visits_by_day  \n",
      "0  2018-01-08T00:00:00-06:00              5.0   [0,1,1,0,1,0,2]  \n",
      "1  2018-01-08T00:00:00-06:00              2.0   [0,0,1,0,1,0,0]  \n",
      "2  2018-01-08T00:00:00-05:00              2.0   [0,1,0,0,0,0,1]  \n",
      "3  2018-01-08T00:00:00-05:00              8.0   [0,2,1,0,2,2,1]  \n",
      "4  2018-01-08T00:00:00-08:00             37.0  [3,8,7,7,10,1,1]  \n"
     ]
    }
   ],
   "source": [
    "#1a\n",
    "import pandas as pd\n",
    "filename = [\n",
    "    '/Users/shishirporeddy/ECON433/weekly_patterns_2018_sample.csv.gz',\n",
    "    '/Users/shishirporeddy/ECON433/weekly_patterns_2019_sample.csv.gz',\n",
    "    '/Users/shishirporeddy/ECON433/weekly_patterns_2020_sample.csv.gz',\n",
    "    '/Users/shishirporeddy/ECON433/weekly_patterns_2021_sample.csv.gz',\n",
    "    '/Users/shishirporeddy/ECON433/weekly_patterns_2022_sample.csv.gz'\n",
    "]\n",
    "row_dictionary = {}\n",
    "jj_list = []\n",
    "\n",
    "for file in filename:\n",
    "    datax = pd.read_csv(file, compression='gzip', low_memory= False)\n",
    "    row_dictionary[file] = len(datax)\n",
    "    print(datax.columns)\n",
    "    jj_data = datax[datax['brands'] ==\"Jimmy John's\"]\n",
    "    jj_list.append(jj_data)\n",
    "count = 0\n",
    "    \n",
    "for file, row in row_dictionary.items():\n",
    "    print(f\"{file}: {count} rows\")\n",
    "    \n",
    "row_tot = sum(row_dictionary.values())\n",
    "print(row_tot)\n",
    "\n",
    "concat_jj_data = pd.concat(jj_list, ignore_index=True)\n",
    "print(concat_jj_data.head())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889e696b-e7ab-48a4-bcd5-a74ae228a65d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104400\n",
      "placekey               object\n",
      "safegraph_brand_ids    object\n",
      "brands                 object\n",
      "top_category           object\n",
      "naics_code             object\n",
      "city                   object\n",
      "region                 object\n",
      "postal_code            object\n",
      "date_range_start       object\n",
      "date_range_end         object\n",
      "raw_visit_counts       object\n",
      "visits_by_day          object\n",
      "dtype: object\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "#1b & 1c\n",
    "print(len(concat_jj_data))\n",
    "print(concat_jj_data.dtypes)\n",
    "\n",
    "jj_unique = concat_jj_data['placekey'].nunique()\n",
    "print(jj_unique)\n",
    "\n",
    "concat_jj_data.to_csv('/Users/shishirporeddy/ECON433/jimmy_johns_wide_sample.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee60d36-f4d3-4d6f-a8b6-0ce988b4d5f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Question 1a\n",
    "- There are a total of 1055229 observations in each individual dataset and a total of 5196505 observations among all the datasets.\n",
    "- The unit of observation in my dataset is a specific restauarant represented by 'placekey' during a designated start and end date\n",
    "\n",
    "Question 1b:\n",
    "- There are now a total of 104400 observations\n",
    "\n",
    "Question 1c: concat_jj_data = \"wide sample\"\n",
    "- The data type is an \"object\" type in every variable in my wide sample\n",
    "- There a total of 400 unique Jimmy Johh's locations in the US\n",
    "- Online shows that there are roughly 2600 Jimmy John's locations, which could mean that my data might be incomplete and hasn't gathered data from all the states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa2102-e0b4-44dd-a3cd-c195c19063bb",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dead391-9583-4e2f-9579-7f42b7eec84b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placekey                       object\n",
      "safegraph_brand_ids            object\n",
      "brands                         object\n",
      "top_category                   object\n",
      "naics_code                     object\n",
      "city                           object\n",
      "region                         object\n",
      "postal_code                    object\n",
      "date_range_start       datetime64[ns]\n",
      "date_range_end         datetime64[ns]\n",
      "raw_visit_counts               object\n",
      "visits_by_day                  object\n",
      "dtype: object\n",
      "2018-01-01 05:00:00\n",
      "2023-01-02 08:00:00\n"
     ]
    }
   ],
   "source": [
    "#2a\n",
    "import datetime\n",
    "concat_jj_data['date_range_start'] = pd.to_datetime(concat_jj_data['date_range_start'], utc=True).dt.tz_localize(None)\n",
    "concat_jj_data['date_range_end'] = pd.to_datetime(concat_jj_data['date_range_end'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "print(concat_jj_data.dtypes)\n",
    "\n",
    "start_date= concat_jj_data['date_range_start'].min()\n",
    "end_date= concat_jj_data['date_range_end'].max()\n",
    "\n",
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0131e83f-f70b-408a-b398-80dfa6a67bfa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "#2b\n",
    "concat_jj_data[['dailyvisits1','dailyvisits2', 'dailyvisits3', 'dailyvisits4', 'dailyvisits5', 'dailyvisits6', 'dailyvisits7']] = concat_jj_data['visits_by_day'].str.strip(\"[]\").str.split(\",\", expand=True).apply(pd.to_numeric, errors='coerce')\n",
    "print(len(concat_jj_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba3b1e19-7313-4749-8999-e2af941f6e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730800\n"
     ]
    }
   ],
   "source": [
    "#2c\n",
    "jj_long = concat_jj_data.melt(\n",
    "    id_vars=['placekey', 'date_range_start', 'date_range_end', 'region', 'brands', 'city', 'raw_visit_counts', 'visits_by_day'],\n",
    "    value_vars=['dailyvisits1','dailyvisits2', 'dailyvisits3', 'dailyvisits4', 'dailyvisits5', 'dailyvisits6', 'dailyvisits7'],\n",
    "    var_name='visits_column',\n",
    "    value_name='dailyvisits')\n",
    "\n",
    "jj_long['dailyvisits'] = pd.to_numeric(jj_long['dailyvisits'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(len(jj_long))\n",
    "\n",
    "jj_long.to_csv(\"/Users/shishirporeddy/ECON433/jimmy_johns_long_sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4578f0bc-f93c-4637-8fdf-c79ff5ded964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    730800.000000\n",
      "mean         15.393637\n",
      "std         156.931828\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           3.000000\n",
      "75%           7.000000\n",
      "max        5915.000000\n",
      "Name: dailyvisits, dtype: float64\n",
      "49776\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#2d\n",
    "dailyvisits_sum_stats = jj_long['dailyvisits'].describe()\n",
    "print(dailyvisits_sum_stats)\n",
    "\n",
    "Q_one = jj_long['dailyvisits'].quantile(0.25)\n",
    "Q_three = jj_long['dailyvisits'].quantile(0.75)\n",
    "inter_quart_range = Q_three - Q_one\n",
    "\n",
    "low_outlier_range = Q_one - 1.5 * inter_quart_range\n",
    "upp_outlier_range = Q_three + 1.5 * inter_quart_range\n",
    "\n",
    "outliers = jj_long[~jj_long['dailyvisits'].between(low_outlier_range, upp_outlier_range)]      \n",
    "print(len(outliers))\n",
    "\n",
    "missing_values = jj_long['dailyvisits'].isna().sum()\n",
    "print(missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a08a34-381a-4c4a-ba5c-5ede76aabbd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Question 2a:\n",
    "The start date is January 1, 2018 at 5:00 am and the end date is January 2, 2023 at 8:00am for my wide sample.\n",
    "\n",
    "Question 2b:\n",
    "There are now 19 variables in the wide sample now. \n",
    "\n",
    "Question 2c:\n",
    "There are now 730,800 observations in the long sample. \n",
    "The pros for the long sample is that it is much easier to analyze the data by day instead of by week. The cons of the long sample are the extra storage and power used since there are more rows. \n",
    "After converting dailyvisits directly into an integer the code doesn't work because you can't convert (NA or inf) to integer, so I added .nafill(0) to resolve this issue. \n",
    "There are still 730,800 observations. \n",
    "\n",
    "Question 2d:\n",
    "Mean = 15.39, STD = 156.93, minimum = 0, maximum = 5915\n",
    "Using interquartile range, I found there to be 49,776 outliers. \n",
    "There are no missing values in dailyvisits. \n",
    "The economic reason for these missing values could simply be due to holiday closures. The data could tell us this by seeing when exactly the misssing values occured and if they land during holiday seasons. \n",
    "Outliers and missing data can result in the business owner to read a different story than what the data is actually saying. One example of a sensitive issue would be in restauarants and trying to schedule the staff on a day by day basis. One example of an immune issue would be something longterm, such as marketing campaings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714feb1-9159-4d46-8098-d6ecac95cd25",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d7ccb1-b3a0-4cc7-a369-e20bcc2a6af9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year          start_date            end_date  observations  mean_visits\n",
      "0  2018 2018-01-01 05:00:00 2019-01-07 08:00:00        148400     8.461044\n",
      "1  2019 2019-01-07 05:00:00 2020-01-06 08:00:00        145600    23.857225\n",
      "2  2020 2020-01-06 05:00:00 2021-01-04 08:00:00        145600    11.465797\n",
      "3  2021 2021-01-04 05:00:00 2022-01-03 08:00:00        145600    14.197562\n",
      "4  2022 2022-01-03 05:00:00 2023-01-02 08:00:00        145600    19.119876\n"
     ]
    }
   ],
   "source": [
    "#3a:\n",
    "jj_long['date_range_start'] = pd.to_datetime(jj_long['date_range_start'], errors = 'coerce')\n",
    "jj_long['year'] = jj_long['date_range_start'].dt.year\n",
    "\n",
    "sum_of_yr = (jj_long.groupby('year', as_index=False).agg(start_date=('date_range_start', 'min'), end_date=('date_range_end', 'max'), observations=('dailyvisits', 'count'), mean_visits=('dailyvisits', 'mean')))\n",
    "\n",
    "print(sum_of_yr)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e7a610-cd7b-4e4f-ad71-6007154df440",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     date_range_start dayofweek\n",
      "0 2018-01-01 06:00:00    Monday\n",
      "1 2018-01-01 06:00:00    Monday\n",
      "2 2018-01-01 05:00:00    Monday\n",
      "3 2018-01-01 05:00:00    Monday\n",
      "4 2018-01-01 08:00:00    Monday\n",
      "1\n",
      "  dayofweek  observations  mean_dailyvisits\n",
      "0    Monday        730800         15.393637\n",
      "Monday\n"
     ]
    }
   ],
   "source": [
    "#3b\n",
    "jj_long['dayofweek'] = jj_long['date_range_start'].dt.day_name()\n",
    "\n",
    "print(jj_long[[ 'date_range_start', 'dayofweek']].head())\n",
    "\n",
    "unique_days = jj_long['dayofweek'].nunique()\n",
    "print(unique_days)\n",
    "\n",
    "summary_dayofweek=(jj_long.groupby('dayofweek', as_index=False).agg(observations=('dailyvisits', 'count'), mean_dailyvisits=('dailyvisits', 'mean')))\n",
    "print(summary_dayofweek)\n",
    "\n",
    "highest_dv = summary_dayofweek.loc[summary_dayofweek['mean_dailyvisits'].idxmax(), 'dayofweek']\n",
    "print(highest_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e9f98f0-fa78-4989-ac9b-d6db15efd310",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     date_range_start day_name   WKND  threshold  dailyvisits  manyvisits\n",
      "0 2018-01-01 06:00:00   Monday  False         18            0           0\n",
      "1 2018-01-01 06:00:00   Monday  False         18            0           0\n",
      "2 2018-01-01 05:00:00   Monday  False         18            0           0\n",
      "3 2018-01-01 05:00:00   Monday  False         18            0           0\n",
      "4 2018-01-01 08:00:00   Monday  False         18            3           0\n",
      "5 2018-01-01 08:00:00   Monday  False         18            2           0\n",
      "6 2018-01-01 06:00:00   Monday  False         18            1           0\n",
      "7 2018-01-01 08:00:00   Monday  False         18            0           0\n",
      "8 2018-01-01 06:00:00   Monday  False         18            0           0\n",
      "9 2018-01-01 06:00:00   Monday  False         18            0           0\n",
      "city\n",
      "             city  tot_obs  count_manyvisits  percentage_manyvisits\n",
      "0         Abilene     1827                 2               0.001095\n",
      "1         Addison     1827                 0               0.000000\n",
      "2  Airway Heights     1827                 0               0.000000\n",
      "3     Albuquerque     1827                 5               0.002737\n",
      "4      Alexandria     3654                27               0.007389\n",
      "5            Alma     1827                 0               0.000000\n",
      "6        Altamont     1827                 0               0.000000\n",
      "7         Altoona     1827                83               0.045430\n",
      "8          Amelia     1827                 6               0.003284\n",
      "9            Ames     1827                20               0.010947\n",
      "Geographic unit with the highest # is: Chicago with 2611\n",
      "Geographic unit with the highest % is: Little River at 0.9989053092501369\n"
     ]
    }
   ],
   "source": [
    "#3c \n",
    "import numpy as np\n",
    "\n",
    "jj_long['date_range_start'] = pd.to_datetime(jj_long['date_range_start'], errors = 'coerce')\n",
    "jj_long['day_name'] = jj_long['date_range_start'].dt.day_name()\n",
    "jj_long['WKND'] = jj_long['day_name'].isin(['Saturday', 'Sunday'])\n",
    "jj_long['threshold'] = np.where(jj_long['WKND'], 13, 18)\n",
    "jj_long['manyvisits'] = np.where(jj_long['dailyvisits'] > jj_long['threshold'], 1, 0)\n",
    "print(jj_long[['date_range_start', 'day_name', 'WKND', 'threshold', 'dailyvisits', 'manyvisits']].head(10))\n",
    "\n",
    "geographic_var = 'city'\n",
    "geo_summary = (\n",
    "    jj_long.groupby(geographic_var, as_index=False).agg(tot_obs=('manyvisits', 'count'),count_manyvisits=('manyvisits', 'sum')).assign(percentage_manyvisits=lambda df:df['count_manyvisits']/df['tot_obs']))\n",
    "\n",
    "max_count=geo_summary['count_manyvisits'].idxmax()\n",
    "max_percentage=geo_summary['percentage_manyvisits'].idxmax()\n",
    "\n",
    "max_count_region = geo_summary.loc[max_count, geographic_var]\n",
    "max_count_value = geo_summary.loc[max_count, 'count_manyvisits']\n",
    "max_percentage_region = geo_summary.loc[max_percentage, geographic_var]\n",
    "max_percentage_value = geo_summary.loc[max_percentage, 'percentage_manyvisits']\n",
    "\n",
    "print(geographic_var)\n",
    "print(geo_summary.head(10))\n",
    "\n",
    "print(f\"Geographic unit with the highest # is: {max_count_region} with {max_count_value}\")\n",
    "print(f\"Geographic unit with the highest % is: {max_percentage_region} at {max_percentage_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af1ccd3-390d-4a4a-9347-4ddd87a1819d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   core_biz_area  observations  mean_dailyvisits\n",
      "0              0        314244         22.316194\n",
      "1              1        416556         10.171358\n"
     ]
    }
   ],
   "source": [
    "#3d\n",
    "\n",
    "core_qualification = 15\n",
    "\n",
    "\n",
    " \n",
    "unique_in_region = (\n",
    "    jj_long.groupby('region', as_index=False)['placekey'].nunique().rename(columns={'placekey':'number_of_stores'}))\n",
    "\n",
    "jj_long = jj_long.merge(unique_in_region, on = 'region', how='left')\n",
    "\n",
    "jj_long['core_biz_area'] = np.where(jj_long['number_of_stores'] >= core_qualification, 1, 0)\n",
    "\n",
    "core_sum =(\n",
    "    jj_long.groupby('core_biz_area', as_index = False).agg(observations = ('dailyvisits', 'count'), mean_dailyvisits=('dailyvisits', 'mean')))\n",
    "print(core_sum)\n",
    "\n",
    "jj_long.to_csv(\"jimmy_johns_long_sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821415c-af8a-48bc-aa53-24d2dfe79557",
   "metadata": {
    "tags": []
   },
   "source": [
    "Question 3a:\n",
    "Generated table summarizing start and end dates, # of observations in each year, and mean dailyvisits per year\n",
    "\n",
    "Question 3b:\n",
    "The day of week with the highest dailyvisits in my long sample is Monday.\n",
    "\n",
    "Question 3c:\n",
    "I chose this threshold because monday was the highest mean per day of week, so I set a threshold of 18 because it seems weekdays are busier than weekends. \n",
    "Chicago is the highest number of observations with manyvisits = 1. \n",
    "Little River has the highest percentage of observations with manyvisits = 1. \n",
    "This data makes sense in real life because Illinois has the most number of locations in the U.S.\n",
    "\n",
    "Question 3d:\n",
    "The number of observations for non-core areas is 314, 244 and for core areas it is 416,556. The mean daily visits for non-core areas is 22.32 and for core areas it is 10.17. \n",
    "On average I observe more daily visits in non-core areas. \n",
    "One reason for this is because of competition in the area, in non-core areas the jimmy john's might be the only sandwhich shop in the area meaning anytime someone wants a sandwhich they'll go to Jimmy John's where as in the core areas they might be other options like subway, potbelly, or jersey mikes which could lead them to have a lower daily average. Another reason might be due to pricing differences, in core areas the price of sandwhiches at Jimmy Johns might be more expensive, while at non-core areas the price could be cheaper. This could lead more people to enjoy jimmy johns in non-core areas because of its lower price point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "133f5d5a-0587-4422-a6b3-93e5f0a1e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in day_of_week: ['Monday' 'Tuesday' 'Wednesday' 'Thursday' 'Friday' 'Saturday' 'Sunday']\n",
      "Weekend observations: 208800\n",
      "Weekday observations: 522000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose you melted into a column called 'visits_column' with values like 'dailyvisits1'..'dailyvisits7'\n",
    "# 1) Extract the day index (1..7)\n",
    "jj_long[\"day_index\"] = (\n",
    "    jj_long[\"visits_column\"].str.extract(r'(\\d+)$')  # capture the digit at the end\n",
    "                   .astype(int)\n",
    ")\n",
    "\n",
    "# 2) Create a true daily date by adding (day_index - 1) days to the start date\n",
    "jj_long[\"true_date\"] = jj_long[\"date_range_start\"] + pd.to_timedelta(jj_long[\"day_index\"] - 1, unit=\"D\")\n",
    "\n",
    "# 3) Compute the actual weekday name\n",
    "jj_long[\"day_of_week\"] = jj_long[\"true_date\"].dt.day_name()\n",
    "\n",
    "# Now you'll see Monday for day_index=1, Tuesday for day_index=2, etc.\n",
    "unique_days = jj_long[\"day_of_week\"].unique()\n",
    "print(\"Unique values in day_of_week:\", unique_days)\n",
    "\n",
    "# 2) Generate weekend variable\n",
    "jj_long['WKND'] = jj_long['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\n",
    "# 3) Count weekend vs. weekday observations\n",
    "weekend_count = (jj_long['WKND'] == 1).sum()\n",
    "weekday_count = (jj_long['WKND'] == 0).sum()\n",
    "\n",
    "print(\"Weekend observations:\", weekend_count)\n",
    "print(\"Weekday observations:\", weekday_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed471d-09b5-43f8-918b-97b7b29dfd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
